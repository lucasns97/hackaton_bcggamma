{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f939869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b3dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "SINAN_PATH      = './../data/raw_data/SINAN_prep_05.csv'\n",
    "MUNICIPIOS_PATH = './../data/municipios_prep_02.csv'\n",
    "BOLSA_PATH = './../data/consolidada_bolsafamilia.csv'\n",
    "INEP_PATH = './../data/consolidada_inep.csv'\n",
    "ATLAS_PATH = './../data/atlas_desenvolvimento_humano_por_municipio.csv'\n",
    "OCUPACOES_PATH  = './../data/cbo_ocupacoes.csv'\n",
    "MAPBOX_TOKEN    = 'pk.eyJ1IjoibHVjYXNuc2VxIiwiYSI6ImNrb241dHZ0cTBpd2MycW5yMGp2enFtMmkifQ.N6NJGlWhG-iYrIJMQ1MVVw'\n",
    "\n",
    "px.set_mapbox_access_token(MAPBOX_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77edbe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "municipios_df = pd.read_csv(MUNICIPIOS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "municipios_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ba734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(values):\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    encoded = encoder.fit_transform(values.reshape(-1, 1))\n",
    "    \n",
    "    return encoded, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(municipios_df, x='pop_2017', marginal=\"box\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8547bda8",
   "metadata": {},
   "source": [
    "### Pipeline de preparação dos dados de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171eba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_data_pipeline(dataframe, train_size = 0.7):\n",
    "    '''\n",
    "    Retorna X_df_train, Y_df_train, X_df_test e Y_df_test\n",
    "    '''\n",
    "    \n",
    "    values_df = dataframe.set_index('id')\n",
    "    \n",
    "    # Removemdo colunas indesejadas\n",
    "    remove_cols = ['id', 'nome', 'uf_nome', 'uf_id', 'sinan_id', 'cluster_id']\n",
    "    \n",
    "    for col in remove_cols:\n",
    "        \n",
    "        try:\n",
    "            values_df = values_df.drop(col, axis=1)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    # Selecionando intervalo de população\n",
    "    \n",
    "    min_population = 10000\n",
    "    max_population = 500000\n",
    "    values_df      = values_df[values_df['pop_2017'] > min_population]\n",
    "    values_df      = values_df[values_df['pop_2017'] < max_population]\n",
    "    \n",
    "    # Encoders em algumas colunas\n",
    "    \n",
    "    encoders = {}\n",
    "    encodeds = {}\n",
    "    feature_columns = ['uf', 'regiao']\n",
    "\n",
    "    for column in feature_columns:\n",
    "\n",
    "        encodeds[column], encoders[column] = get_encoder(values_df[column].values)\n",
    "        values_df[column] = encodeds[column]\n",
    "        \n",
    "    # Fillna\n",
    "    values_df = values_df.fillna(-1)\n",
    "    \n",
    "    # Removendo outras colunas excessivas\n",
    "    \n",
    "#     remove_cols = []\n",
    "#     for col in values_df.columns:\n",
    "#         if 'icg' in col or 'comodos' in col:\n",
    "#             remove_cols.append(col)\n",
    "    \n",
    "#     values_df = values_df.drop(remove_cols, axis=1)\n",
    "    \n",
    "    # Reduzindo à colunas específicas\n",
    "    keep_columns = ['denun_relat_2016', 'denun_relat_2015', 'denun_relat_2014', 'denun_relat_2013',\n",
    "                    'denun_maioridade_relat_2016', 'denun_relacao_relat_2016', \n",
    "                    'denun_maioridade_relat_2015', 'denun_relacao_relat_2015', 'pop_2016',\n",
    "                    'denun_count_2016', 'denun_count_2013', 'renda_media_sum_2016', 'renda_media_sum_2013', \n",
    "                    'latitude', 'regiao', 'share_agua_canalizada_2016', 'share_agua_canalizada_2013', \n",
    "                    'renda_media_median_2016', 'renda_media_median_2013', 'denun_relat_2017']\n",
    "    \n",
    "    values_df = values_df[keep_columns]\n",
    "    \n",
    "    # Removendo colunas de 2017 e dividindo entre X_df e Y_df\n",
    "    \n",
    "    remove_cols = []\n",
    "    for col in values_df.columns:\n",
    "        if '2017' in col:\n",
    "            remove_cols.append(col)\n",
    "    \n",
    "    X_df = values_df.drop(remove_cols, axis=1)\n",
    "    Y_df = values_df['denun_relat_2017']\n",
    "    \n",
    "    # Separando df em train/test\n",
    "    train_indexes = list(np.random.choice(X_df.index, int(len(X_df)*train_size), replace = False))\n",
    "    test_indexes  = []\n",
    "    \n",
    "    for index in X_df.index:\n",
    "        if index not in train_indexes:\n",
    "            test_indexes.append(index)\n",
    "    \n",
    "    X_df_train = X_df.drop(test_indexes, axis=0)\n",
    "    Y_df_train = Y_df.drop(test_indexes, axis=0)\n",
    "    X_df_test  = X_df.drop(train_indexes, axis=0)\n",
    "    Y_df_test  = Y_df.drop(train_indexes, axis=0)\n",
    "    \n",
    "    return X_df_train, Y_df_train, X_df_test, Y_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8679ac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_train_result(X_df, Y_df, regressor, original_df):\n",
    "    \n",
    "    X_values = X_df.values\n",
    "    Y_values = Y_df.values.reshape(-1, 1)\n",
    "    \n",
    "    predictions_df = pd.DataFrame()\n",
    "    \n",
    "    predictions_df['prediction'] = regressor.predict(X_values).reshape(-1,)\n",
    "    predictions_df['real']       = Y_values.reshape(-1,)\n",
    "\n",
    "    \n",
    "    predictions_df['abs_diff']   = np.abs(predictions_df['prediction'] - predictions_df['real'])\n",
    "    predictions_df['diff']       = predictions_df['prediction'] - predictions_df['real']\n",
    "    predictions_df['sqr_diff']   = (predictions_df['prediction'] - predictions_df['real'])**2\n",
    "    \n",
    "    variables           = ['nome', 'uf', 'regiao', 'pop_2016']\n",
    "    variables_values    = {}\n",
    "    original_df_indexed = original_df.set_index('id')\n",
    "    \n",
    "    for variable in variables:\n",
    "        variables_values[variable] = []\n",
    "        \n",
    "    for mun_id in X_df.index:\n",
    "\n",
    "        mun_data = original_df_indexed.loc[mun_id]\n",
    "        \n",
    "        for variable in variables:\n",
    "            variables_values[variable].append(mun_data[variable])\n",
    "    \n",
    "    for variable in variables:\n",
    "        predictions_df[variable] = variables_values[variable]\n",
    "        \n",
    "    score = regressor.score(X_values, Y_values)\n",
    "    name  = str(regressor).split('()')[0]\n",
    "    \n",
    "    title = f'Regressor: {name} | score: {round(score, 5)}'\n",
    "    \n",
    "\n",
    "    fig = px.scatter(predictions_df, x='prediction', y='real', color=\"abs_diff\", \n",
    "                     hover_data=variables + ['abs_diff', 'diff', 'sqr_diff'], title=title)\n",
    "    \n",
    "    max_val = np.max(predictions_df['prediction'].values)\n",
    "    min_val = np.min(predictions_df['prediction'].values)\n",
    "    \n",
    "    fig.add_trace(go.Line(x=np.linspace(min_val*0.9,max_val*1.1), y=np.linspace(min_val*0.9,max_val*1.1)))\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d896b32c",
   "metadata": {},
   "source": [
    "### Aplicando alguns regressores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a5b731",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176c50f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_df_train, Y_df_train, X_df_test, Y_df_test = train_test_data_pipeline(municipios_df, train_size = 0.7)\n",
    "\n",
    "X_values_train, Y_value_train = X_df_train.values, Y_df_train.values.reshape(-1,)\n",
    "X_values_test, Y_values_test  = X_df_test.values, Y_df_test.values.reshape(-1,)\n",
    "\n",
    "lr_regressor = LinearRegression()\n",
    "\n",
    "lr_regressor = lr_regressor.fit(X_values_train, Y_value_train)\n",
    "score     = lr_regressor.score(X_df_test, Y_df_test)\n",
    "\n",
    "print(f'Train Size: {round(100*len(X_df_train)/len(municipios_df), 2)}% | {len(X_df_train)}/{len(municipios_df)}')\n",
    "print('Score:', round(score, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae629d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_train_result(X_df_test, Y_df_test, lr_regressor, municipios_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b367e906",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85fc14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_df_train, Y_df_train, X_df_test, Y_df_test = train_test_data_pipeline(municipios_df, train_size = 0.7)\n",
    "\n",
    "X_values_train, Y_value_train = X_df_train.values, Y_df_train.values.reshape(-1,)\n",
    "X_values_test, Y_values_test  = X_df_test.values, Y_df_test.values.reshape(-1,)\n",
    "\n",
    "rf_regressor = RandomForestRegressor()\n",
    "\n",
    "rf_regressor = rf_regressor.fit(X_values_train, Y_value_train)\n",
    "score     = rf_regressor.score(X_df_test, Y_df_test)\n",
    "\n",
    "print(f'Train Size: {round(100*len(X_df_train)/len(municipios_df), 2)}% | {len(X_df_train)}/{len(municipios_df)}')\n",
    "print('Score:', round(score, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_train_result(X_df_test, Y_df_test, rf_regressor, municipios_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8340be",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cedf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_df_train, Y_df_train, X_df_test, Y_df_test = train_test_data_pipeline(municipios_df, train_size = 0.7)\n",
    "\n",
    "X_values_train, Y_value_train = X_df_train.values, Y_df_train.values.reshape(-1,)\n",
    "X_values_test, Y_values_test  = X_df_test.values, Y_df_test.values.reshape(-1,)\n",
    "\n",
    "xg_regressor = GradientBoostingRegressor()\n",
    "\n",
    "xg_regressor = xg_regressor.fit(X_values_train, Y_value_train)\n",
    "score        = xg_regressor.score(X_df_test, Y_df_test)\n",
    "\n",
    "print(f'Train Size: {round(100*len(X_df_train)/len(municipios_df), 2)}% | {len(X_df_train)}/{len(municipios_df)}')\n",
    "print('Score:', round(score, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430cc7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_train_result(X_df_test, Y_df_test, xg_regressor, municipios_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62061255",
   "metadata": {},
   "source": [
    "#### Avaliando Capitais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285d907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals = ['Rio Branco','Maceió','Macapá','Manaus','Salvador','Fortaleza','Brasília','Vitória',\n",
    "            'Goiânia','São Luís','Cuiabá','Campo Grande','Belo Horizonte','Belém','João Pessoa',\n",
    "            'Curitiba','Recife','Teresina','Rio de Janeiro','Natal','Porto Alegre','Porto Velho',\n",
    "            'Boa Vista','Florianópolis','São Paulo','Aracaju','Palmas']\n",
    "\n",
    "mun_data = municipios_df[municipios_df['nome'].isin(capitals)]\n",
    "\n",
    "X_df, Y_df, _, _ = train_test_data_pipeline(mun_data, train_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4294d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_train_result(X_df, Y_df, lr_regressor, municipios_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78539ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_train_result(X_df, Y_df, rf_regressor, municipios_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6722c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_train_result(X_df, Y_df, xg_regressor, municipios_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87495f96",
   "metadata": {},
   "source": [
    "#### Avaliando base toda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b9784",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df, Y_df, _, _ = train_test_data_pipeline(municipios_df, train_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1edcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_train_result(X_df, Y_df, lr_regressor, municipios_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6315707",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_train_result(X_df, Y_df, rf_regressor, municipios_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef8b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_train_result(X_df, Y_df, xg_regressor, municipios_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2410ea6",
   "metadata": {},
   "source": [
    "### Facet Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b5596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "# some helpful imports from sklearndf\n",
    "from sklearndf.pipeline import RegressorPipelineDF\n",
    "from sklearndf.regression import RandomForestRegressorDF\n",
    "\n",
    "# relevant FACET imports\n",
    "from facet.data import Sample\n",
    "from facet.selection import LearnerRanker, LearnerGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6467921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_train, Y_df_train, X_df_test, Y_df_test = train_test_data_pipeline(municipios_df, train_size = 1)\n",
    "\n",
    "df = X_df_train.copy()\n",
    "df['denun_relat_2017'] = Y_df_train.values\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46017cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create FACET sample object\n",
    "df_sample = Sample(observations=df, target_name=\"denun_relat_2017\")\n",
    "\n",
    "# create a (trivial) pipeline for a random forest regressor\n",
    "rnd_forest_reg = RegressorPipelineDF(\n",
    "    regressor=RandomForestRegressorDF(n_estimators=200, random_state=42)\n",
    ")\n",
    "\n",
    "# define grid of models which are \"competing\" against each other\n",
    "rnd_forest_grid = [\n",
    "    LearnerGrid(\n",
    "        pipeline=rnd_forest_reg,\n",
    "        learner_parameters={\n",
    "            \"min_samples_leaf\": [8, 11, 15],\n",
    "            \"max_depth\": [4, 5, 6],\n",
    "        }\n",
    "    ),\n",
    "]\n",
    "\n",
    "# create repeated k-fold CV iterator\n",
    "rkf_cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "\n",
    "# rank your candidate models by performance (default is mean CV score - 2*SD)\n",
    "ranker = LearnerRanker(\n",
    "    grids=rnd_forest_grid, cv=rkf_cv, n_jobs=-3\n",
    ").fit(sample=df_sample)\n",
    "\n",
    "# get summary report\n",
    "ranker.summary_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e3117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model inspector\n",
    "from facet.inspection import LearnerInspector\n",
    "inspector = LearnerInspector(n_jobs=-3)\n",
    "inspector.fit(crossfit=ranker.best_model_crossfit_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10919da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise synergy as a matrix\n",
    "from pytools.viz.matrix import MatrixDrawer\n",
    "synergy_matrix = inspector.feature_synergy_matrix()\n",
    "px.imshow(synergy_matrix, width = 700, height = 700, title = 'Sinergy Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b67a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise redundancy as a matrix\n",
    "redundancy_matrix = inspector.feature_redundancy_matrix()\n",
    "px.imshow(redundancy_matrix, width = 700, height = 700, title = 'Redudancy Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef755cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise redundancy using a dendrogram\n",
    "from pytools.viz.dendrogram import DendrogramDrawer\n",
    "import matplotlib.pyplot as plt\n",
    "redundancy = inspector.feature_redundancy_linkage()\n",
    "\n",
    "plt.figure(figsize=(14,12))\n",
    "DendrogramDrawer().draw(data=redundancy, title=\"Redundancy Dendrogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c06e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = ['denun_relat_2016', 'denun_maioridade_relat_2016', 'denun_relacao_2016', \n",
    "                'denun_relat_2015', 'denun_maioridade_relat_2015', 'denun_relacao_2015',\n",
    "                'denun_count_2015', 'denun_relat_2014', 'denun_relat_2013', 'denun_count_2014',\n",
    "                'renda_media_sum_2013', 'renda_media_sum_2016', 'latitude', 'regiao', \n",
    "                'share_agua_canalizada_2013', 'share_agua_canalizada_2016', 'renda_media_median_2013', \n",
    "                'renda_media_median_2016']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb79c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACET imports\n",
    "from facet.validation import BootstrapCV\n",
    "from facet.crossfit import LearnerCrossfit\n",
    "from facet.simulation import UnivariateUpliftSimulator\n",
    "from facet.data.partition import ContinuousRangePartitioner\n",
    "from facet.simulation.viz import SimulationDrawer\n",
    "\n",
    "# create bootstrap CV iterator\n",
    "bscv = BootstrapCV(n_splits=1000, random_state=42)\n",
    "\n",
    "# create a bootstrap CV crossfit for simulation using best model\n",
    "boot_crossfit = LearnerCrossfit(\n",
    "    pipeline=ranker.best_model_,\n",
    "    cv=bscv,\n",
    "    n_jobs=-3,\n",
    "    verbose=False,\n",
    ").fit(sample=df_sample)\n",
    "\n",
    "SIM_FEAT = \"denun_relat_2016\"\n",
    "simulator = UnivariateUpliftSimulator(crossfit=boot_crossfit, n_jobs=-3)\n",
    "\n",
    "# split the simulation range into equal sized partitions\n",
    "partitioner = ContinuousRangePartitioner()\n",
    "\n",
    "# run the simulation\n",
    "simulation = simulator.simulate_feature(feature_name=SIM_FEAT, partitioner=partitioner)\n",
    "\n",
    "# visualise results\n",
    "plt.figure(figsize=(14,12))\n",
    "SimulationDrawer().draw(data=simulation, title=SIM_FEAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b605c1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
