{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f939869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from boruta import BorutaPy\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b5596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "# some helpful imports from sklearndf\n",
    "from sklearndf.pipeline import RegressorPipelineDF\n",
    "from sklearndf.regression import RandomForestRegressorDF\n",
    "\n",
    "# relevant FACET imports\n",
    "from facet.data import Sample\n",
    "from facet.selection import LearnerRanker, LearnerGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b3dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "SINAN_PATH      = './../data/raw_data/SINAN_prep_05.csv'\n",
    "MUNICIPIOS_PATH = './../data/municipios_prep_06.csv'\n",
    "BOLSA_PATH = './../data/consolidada_bolsafamilia.csv'\n",
    "INEP_PATH = './../data/consolidada_inep.csv'\n",
    "ATLAS_PATH = './../data/atlas_desenvolvimento_humano_por_municipio.csv'\n",
    "OCUPACOES_PATH  = './../data/cbo_ocupacoes.csv'\n",
    "PNAD_PATH  = './../data/PNAD_consolidado.csv'\n",
    "MAPBOX_TOKEN    = 'pk.eyJ1IjoibHVjYXNuc2VxIiwiYSI6ImNrb241dHZ0cTBpd2MycW5yMGp2enFtMmkifQ.N6NJGlWhG-iYrIJMQ1MVVw'\n",
    "\n",
    "px.set_mapbox_access_token(MAPBOX_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77edbe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "municipios_df = pd.read_csv(MUNICIPIOS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "municipios_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a982f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "municipios_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ba734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(values):\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    encoded = encoder.fit_transform(values.ravel())\n",
    "    \n",
    "    return encoded, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(municipios_df, x='pop_2017', marginal=\"box\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = municipios_df[(municipios_df['pop_2017'] >= 10000) &  (municipios_df['pop_2017'] <= 500000)]\n",
    "fig = px.histogram(df, x='denun_relat_2017', marginal=\"box\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8547bda8",
   "metadata": {},
   "source": [
    "### Pipeline de preparação dos dados de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc29189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_munic_nan_indexes(dataframe):\n",
    "    \n",
    "    munic_cols = []\n",
    "\n",
    "    for col in dataframe.columns:\n",
    "        if 'munic' in col:\n",
    "            munic_cols.append(col)\n",
    "            \n",
    "    for i, col in enumerate(munic_cols):\n",
    "        \n",
    "        if i == 0:\n",
    "            df = dataframe[dataframe[col].notna()]\n",
    "        else:\n",
    "            df = df[df[col].notna()]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8679ac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_train_result(X_df, Y_df, regressor, original_df, years):\n",
    "    \n",
    "    X_values = X_df.values\n",
    "    Y_values = Y_df.values.reshape(-1, 1)\n",
    "    \n",
    "    predictions_df = pd.DataFrame()\n",
    "    \n",
    "    predictions_df['prediction'] = regressor.predict(X_values).reshape(-1,)\n",
    "    predictions_df['real']       = Y_values.reshape(-1,)\n",
    "\n",
    "    \n",
    "    predictions_df['abs_diff']   = np.abs(predictions_df['prediction'] - predictions_df['real'])\n",
    "    predictions_df['abs_diff_10k']   = np.abs(predictions_df['prediction'] - predictions_df['real'])*10000\n",
    "    predictions_df['diff_10k']   = (predictions_df['prediction'] - predictions_df['real'])*10000\n",
    "    predictions_df['diff']       = predictions_df['prediction'] - predictions_df['real']\n",
    "    predictions_df['sqr_diff']   = (predictions_df['prediction'] - predictions_df['real'])**2\n",
    "    \n",
    "    variables           = ['nome', 'uf', 'regiao', 'pop', 'year']\n",
    "    variables_values    = {}\n",
    "    \n",
    "    original_df_indexed = original_df.set_index('id')\n",
    "    \n",
    "    for variable in variables:\n",
    "        variables_values[variable] = []\n",
    "        \n",
    "    for index in X_df.index:\n",
    "        \n",
    "        index_parts = index.split('_')\n",
    "        mun_id, year = int(index_parts[0]), int(index_parts[1])\n",
    "\n",
    "        mun_data = original_df_indexed.loc[mun_id]\n",
    "        \n",
    "        for variable in variables:\n",
    "            \n",
    "            if variable == 'pop':\n",
    "                variables_values[variable].append(mun_data[f'pop_{year}'])\n",
    "            elif variable == 'year':\n",
    "                variables_values[variable].append(year)\n",
    "            else:\n",
    "                variables_values[variable].append(mun_data[variable])\n",
    "    \n",
    "    for variable in variables:\n",
    "        predictions_df[variable] = variables_values[variable]\n",
    "        \n",
    "    score = regressor.score(X_values, Y_values)\n",
    "    rmse  = np.sqrt(mean_squared_error(predictions_df['real'].values, predictions_df['prediction'].values))\n",
    "    \n",
    "    name  = str(regressor).split('(')[0]\n",
    "    \n",
    "    title = f'Regressor: {name} | Score: {round(score, 3)} | RMSE: {round(rmse, 6)}'\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    size   = scaler.fit_transform(predictions_df['year'].values.reshape(-1, 1)).reshape(-1)\n",
    "    col_year = predictions_df['year'].apply(lambda x : f'y_{x}')\n",
    "\n",
    "    fig = px.scatter(predictions_df, x='prediction', y='real', color='diff', size=size, size_max=7,\n",
    "                     hover_data=variables + ['abs_diff', 'diff', 'sqr_diff'], title=title)\n",
    "    \n",
    "    max_val = np.max(predictions_df['prediction'].values)\n",
    "    min_val = np.min(predictions_df['prediction'].values)\n",
    "    \n",
    "    fig.add_trace(go.Line(x=np.linspace(min_val*0.9,max_val*1.1), y=np.linspace(min_val*0.9,max_val*1.1)))\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b08c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_df(X_df, Y_df, regressor, original_df, years):\n",
    "    \n",
    "    X_values = X_df.values\n",
    "    Y_values = Y_df.values.reshape(-1, 1)\n",
    "    \n",
    "    predictions_df = pd.DataFrame()\n",
    "    \n",
    "    predictions_df['prediction'] = regressor.predict(X_values).reshape(-1,)\n",
    "    predictions_df['real']       = Y_values.reshape(-1,)\n",
    "\n",
    "    \n",
    "    predictions_df['abs_diff']   = np.abs(predictions_df['prediction'] - predictions_df['real'])\n",
    "    predictions_df['abs_diff_10k']   = np.abs(predictions_df['prediction'] - predictions_df['real'])*10000\n",
    "    predictions_df['diff_10k']   = (predictions_df['prediction'] - predictions_df['real'])*10000\n",
    "    predictions_df['diff']       = predictions_df['prediction'] - predictions_df['real']\n",
    "    predictions_df['sqr_diff']   = (predictions_df['prediction'] - predictions_df['real'])**2\n",
    "    \n",
    "    variables           = ['nome', 'uf', 'regiao', 'pop', 'year', 'latitude', 'longitude']\n",
    "    variables_values    = {}\n",
    "    \n",
    "    original_df_indexed = original_df.set_index('id')\n",
    "    \n",
    "    for variable in variables:\n",
    "        variables_values[variable] = []\n",
    "        \n",
    "    for index in X_df.index:\n",
    "        \n",
    "        index_parts = index.split('_')\n",
    "        mun_id, year = int(index_parts[0]), int(index_parts[1])\n",
    "\n",
    "        mun_data = original_df_indexed.loc[mun_id]\n",
    "        \n",
    "        for variable in variables:\n",
    "            \n",
    "            if variable == 'pop':\n",
    "                variables_values[variable].append(mun_data[f'pop_{year}'])\n",
    "            elif variable == 'year':\n",
    "                variables_values[variable].append(year)\n",
    "            else:\n",
    "                variables_values[variable].append(mun_data[variable])\n",
    "    \n",
    "    for variable in variables:\n",
    "        predictions_df[variable] = variables_values[variable]\n",
    "        \n",
    "    score = regressor.score(X_values, Y_values)\n",
    "    rmse  = np.sqrt(mean_squared_error(predictions_df['real'].values, predictions_df['prediction'].values))\n",
    "\n",
    "    return predictions_df, score, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6931eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_selected_data_pipeline(dataframe, years, train_size = 0.7, filter_pop = True, selected_prefix_cols = None, \n",
    "                             ignore_cols = []):\n",
    "    '''\n",
    "    Retorna X_df_train, Y_df_train, X_df_test e Y_df_test\n",
    "    '''\n",
    "    \n",
    "    # se selected cols == None => use all\n",
    "    \n",
    "    values_df = dataframe.set_index('id').drop(columns=['sinan_id', 'nome', 'uf_nome', 'uf_id'])\n",
    "    values_df = values_df.drop(columns=ignore_cols)\n",
    "    \n",
    "    selected_cols = []\n",
    "    if selected_prefix_cols:\n",
    "        \n",
    "        for prefix in selected_prefix_cols:\n",
    "            for col in values_df.columns:\n",
    "                \n",
    "                if prefix in col:\n",
    "                    selected_cols.append(col)\n",
    "        \n",
    "        values_df = values_df[selected_cols]\n",
    "        \n",
    "    # Removendo valores NaN do MUNIC\n",
    "#     values_df = remove_munic_nan_indexes(values_df)\n",
    "    \n",
    "    # Features and Numeric\n",
    "    feature_cols = []\n",
    "    numeric_cols = []\n",
    "    for col in values_df.columns:\n",
    "        \n",
    "        if isinstance(values_df[col].values[0], str):\n",
    "            feature_cols.append(col)\n",
    "        else:\n",
    "            numeric_cols.append(col)\n",
    "    \n",
    "    encodeds, encoders = {}, {}\n",
    "    for column in feature_cols:\n",
    "\n",
    "        encodeds[column], encoders[column] = get_encoder(values_df[column].values)\n",
    "        values_df[column] = encodeds[column]\n",
    "    \n",
    "    # Selecionando colunas que variam com os anos\n",
    "    year_columns = []\n",
    "    for col in values_df.columns:\n",
    "        for year in np.arange(2010,2020):\n",
    "            y_str = str(year)\n",
    "            if y_str in col and col not in year_columns:\n",
    "                year_columns.append(col)\n",
    "                \n",
    "    estatic_columns = []\n",
    "    for col in values_df.columns:\n",
    "        if col not in year_columns:\n",
    "            estatic_columns.append(col)\n",
    "            \n",
    "                    \n",
    "    # Removendo sufixo de ano \"_year\"\n",
    "    renamed_year_columns_map = {}\n",
    "    for col in year_columns:\n",
    "        renamed_year_columns_map[col] = '_'.join(col.split('_')[:-1])\n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    min_population = 10000\n",
    "    max_population = 500000\n",
    "    \n",
    "    for year in years:\n",
    "        \n",
    "        # Selecionando intervalo de população\n",
    "        \n",
    "        if filter_pop:\n",
    "            temp_df = values_df[(values_df[f'pop_{year}'] >= min_population) & (values_df[f'pop_{year}'] <= max_population)]\n",
    "        else:\n",
    "            temp_df = values_df.copy()\n",
    "        \n",
    "        year_df = pd.DataFrame(index = temp_df.index)\n",
    "        \n",
    "        for col in estatic_columns:\n",
    "            year_df[col] = temp_df[col].values\n",
    "            \n",
    "        for col in year_columns:\n",
    "            if str(year) in col:\n",
    "                year_df[col] = temp_df[col].values\n",
    "                \n",
    "        year_df = year_df.rename(renamed_year_columns_map, axis=1)\n",
    "        \n",
    "        year_df = year_df.rename(lambda x : f'{x}_{year}', axis=0)\n",
    "        \n",
    "        year_df = year_df.fillna(0)\n",
    "        \n",
    "        df = pd.concat((df, year_df))\n",
    "        \n",
    "        \n",
    "    X_df = df.drop(f'denun_relat', axis=1)\n",
    "    Y_df = df['denun_relat']\n",
    "    \n",
    "    # Separando df em train/test\n",
    "    train_indexes = X_df.sample(int(len(X_df) * train_size)).index\n",
    "    test_indexes  = []\n",
    "    \n",
    "    for index in X_df.index:\n",
    "        if index not in train_indexes:\n",
    "            test_indexes.append(index)\n",
    "    \n",
    "    X_df_train = X_df.drop(test_indexes, axis=0)\n",
    "    Y_df_train = Y_df.drop(test_indexes, axis=0)\n",
    "    X_df_test  = X_df.drop(train_indexes, axis=0)\n",
    "    Y_df_test  = Y_df.drop(train_indexes, axis=0)\n",
    "    \n",
    "    return X_df_train, Y_df_train, X_df_test, Y_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2119f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cols = ['latitude', 'pnad_espvida', 'pnad_t_freq6a14', 'uf', 'pop',\n",
    "       'renda_media_sum', 'percentual_agressor_namorado/a',\n",
    "       'percentual_agressor_cônjuge', 'percentual_agressor_ex-cônjuge',\n",
    "       'percentual_agressor_ex-namorado/a', 'percentual_agressor_amigo/a',\n",
    "       'percentual_agressor_próprio/a', 'percentual_agressor_pai',\n",
    "       'percentual_agressor_mãe', 'percentual_agressor_padrasto',\n",
    "       'percentual_agressor_filho/a',\n",
    "       'percentual_agressor_policial/agente da lei',\n",
    "       'percentual_violence_sexual', 'percentual_violence_psicológica',\n",
    "       'percentual_escolaridade_ef ii completo',\n",
    "       'percentual_escolaridade_es completo',\n",
    "       'percentual_cbo_grupo_técnicos de nivel médio',\n",
    "       'percentual_cbo_grupo_trabalhadores agropecuários, florestais e da pesca',\n",
    "       'percentual_sit_conjug_separado/a',\n",
    "       'percentual_sit_conjug_viúvo/a',\n",
    "       'percentual_viol_motiv_conflito geracional',\n",
    "       'percentual_cs_sexo_f', 'percentual_age_faixa_25_39',\n",
    "       'percentual_age_faixa_40_59', 'denun_relat']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b367e906",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f07b8b4",
   "metadata": {},
   "source": [
    "#### Prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6491b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2013,2014,2015,2016,2017]\n",
    "\n",
    "X_df_train, Y_df_train, _, _ = train_test_selected_data_pipeline(municipios_df, \n",
    "                                                                 years, \n",
    "                                                                 train_size = 1, \n",
    "                                                                 selected_prefix_cols = cols, \n",
    "                                                                 filter_pop = False)\n",
    "\n",
    "df = X_df_train.copy()\n",
    "df['denun_relat'] = Y_df_train.values\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a47d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create FACET sample object\n",
    "df_sample = Sample(observations=df, target_name=\"denun_relat\")\n",
    "\n",
    "# create a (trivial) pipeline for a random forest regressor\n",
    "rnd_forest_reg = RegressorPipelineDF(\n",
    "    regressor=RandomForestRegressorDF(n_estimators=200, random_state=42)\n",
    ")\n",
    "\n",
    "# define grid of models which are \"competing\" against each other\n",
    "rnd_forest_grid = [\n",
    "    LearnerGrid(\n",
    "        pipeline=rnd_forest_reg,\n",
    "        learner_parameters={\n",
    "            \"min_samples_leaf\": [5, 8],\n",
    "            \"max_depth\": [12, 14, 16, 18],\n",
    "        }\n",
    "    ),\n",
    "]\n",
    "\n",
    "# create repeated k-fold CV iterator\n",
    "rkf_cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "\n",
    "# rank your candidate models by performance (default is mean CV score - 2*SD)\n",
    "ranker = LearnerRanker(\n",
    "    grids=rnd_forest_grid, cv=rkf_cv, n_jobs=-3\n",
    ").fit(sample=df_sample)\n",
    "\n",
    "# get summary report\n",
    "ranker.summary_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e18d1a4",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85fc14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "years = [2013, 2014, 2015, 2016, 2017]\n",
    "\n",
    "X_df_train, Y_df_train, X_df_test, Y_df_test = train_test_selected_data_pipeline(municipios_df, \n",
    "                                                                                 years, \n",
    "                                                                                 train_size = 0.6, \n",
    "                                                                                 selected_prefix_cols = cols, \n",
    "                                                                                 filter_pop = False)\n",
    "\n",
    "X_values_train, Y_value_train = X_df_train.values, Y_df_train.values.reshape(-1,)\n",
    "X_values_test, Y_values_test  = X_df_test.values, Y_df_test.values.reshape(-1,)\n",
    "\n",
    "rf_regressor = RandomForestRegressor(min_samples_leaf=8, max_depth=18)\n",
    "\n",
    "rf_regressor = rf_regressor.fit(X_values_train, Y_value_train)\n",
    "score     = rf_regressor.score(X_df_test, Y_df_test)\n",
    "\n",
    "print(f'Train Size: {round(100*len(X_df_train)/(len(municipios_df)*len(years)), 2)}% | {len(X_df_train)}/{len(municipios_df)*len(years)}')\n",
    "print('Score:', round(score, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_train_result(X_df_test, Y_df_test, rf_regressor, municipios_df, years)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62061255",
   "metadata": {},
   "source": [
    "#### Avaliando Capitais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285d907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2013, 2014, 2015, 2016, 2017]\n",
    "\n",
    "capitals = ['Rio Branco','Maceió','Macapá','Manaus','Salvador','Fortaleza','Brasília','Vitória',\n",
    "            'Goiânia','São Luís','Cuiabá','Campo Grande','Belo Horizonte','Belém','João Pessoa',\n",
    "            'Curitiba','Recife','Teresina','Rio de Janeiro','Natal','Porto Alegre','Porto Velho',\n",
    "            'Boa Vista','Florianópolis','São Paulo','Aracaju','Palmas']\n",
    "\n",
    "mun_data = municipios_df[municipios_df['nome'].isin(capitals)]\n",
    "\n",
    "X_df, Y_df, _, _ = train_test_selected_data_pipeline(mun_data, \n",
    "                                                     years, \n",
    "                                                     train_size = 1, \n",
    "                                                     selected_prefix_cols = cols, \n",
    "                                                     filter_pop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78539ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_train_result(X_df, Y_df, rf_regressor, municipios_df, years)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87495f96",
   "metadata": {},
   "source": [
    "#### Avaliando base toda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b9784",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df, Y_df, _, _ = train_test_selected_data_pipeline(municipios_df, \n",
    "                                                     years, \n",
    "                                                     train_size = 1, \n",
    "                                                     selected_prefix_cols = cols, \n",
    "                                                     filter_pop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6315707",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_train_result(X_df, Y_df, rf_regressor, municipios_df, years)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b9163",
   "metadata": {},
   "source": [
    "#### Mapa de outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe88660",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2013,2014,2015,2016,2017]\n",
    "\n",
    "frame = []\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for year in tqdm(years):\n",
    "    \n",
    "    X_df, Y_df, _, _ = train_test_selected_data_pipeline(municipios_df, \n",
    "                                                         [year], \n",
    "                                                         train_size = 1, \n",
    "                                                         selected_prefix_cols = cols, \n",
    "                                                         filter_pop = True)\n",
    "    predictions_df, score, rmse = get_prediction_df(X_df, Y_df, rf_regressor, municipios_df, [year])\n",
    "    \n",
    "    predictions_df['text'] = predictions_df['nome'] + ', diff = ' + round(predictions_df['diff'], 5).astype(str) + ', ' + predictions_df['year'].astype(str)\n",
    "\n",
    "    df = pd.concat((df, predictions_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../data/predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62311180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['pop'] > 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be0382",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_frames = len(years)\n",
    "\n",
    "\n",
    "fig = ff.create_hexbin_mapbox(\n",
    "    data_frame=df,lat='latitude', lon='longitude', nx_hexagon=90, animation_frame='year', zoom=3.3, \n",
    "    center={'lat': -15.778244, 'lon': -47.906081}, range_color=[-15,15],\n",
    "    labels={\"color\": \"Difererença por 10k hab.\", \"frame\": \"Ano\"}, color_continuous_midpoint=0,\n",
    "    opacity=0.7, color=\"diff_10k\", agg_func=np.mean, color_continuous_scale=\"Temps_r\", width = 650, height=700\n",
    ")\n",
    "fig.update_layout(margin=dict(b=0, t=0, l=0, r=0))\n",
    "fig.layout.sliders[0].pad.t=20\n",
    "fig.layout.updatemenus[0].pad.t=40\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3362f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
